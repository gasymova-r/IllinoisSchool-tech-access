---
title: "Modeling Approach"
author: "Rafa Gasymova"
date: "2023-04-03"
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
    theme: journal
    highlight: pygments
    code_folding: hide
editor_options:
  chunk_output_type: inline
---
# Access to Technology in Illinois School Districts


## Introduction

### Policy context

Several public agencies across the nation started paying more attention to the digital divide present in their communities, especially among students. This was further amplified by the COVID-19 pandemic when student access to technology became crucial for their education progression. According to data as of the year 2020, around 100,000 students in Chicago lacked access to high-speed Internet (“Chicago Launches Groundbreaking Initiative to Bridge Digital Divide,” 2020). That is why in 2020 the City of Chicago pioneered "one of kind" initiative called ["Chicago Connected"](https://www.cps.edu/strategic-initiatives/chicago-connected/) that is aimed at providing local students from low-income communities with high-speed Internet. The public program has already had some success resulting in increased remote learning attendance rates. However, despite its impressive results, this is still a city-level program and there are many other school districts in Illinois that need the support. By implementing this project we are hoping to identify Illinois districts that demonstrate the most urgent need for technology connectivity funding.  We hope that the produced visualizations will convey a message about which exact districts policymakers should concentrate on and why.  

### Research question
> Are there racial and economic disparities in access to technology in Illinois School Districts? In other words, does access to technology in the Illinois School Districts depend on the demographic characteristics of students and how we can showcase that with data?

To answer this we will attempt to build a model predicting schools' technology accessibility levels and then look at whether any socio-economic or racial factors contribute significantly to the accuracy of predictions.

### About the data

The data  comes from the report made by the [Illinois State Board of Education](https://www.isbe.net/Pages/Data-Analysis-Reports.aspx) and can be found on the agency's website. The unit of analysis here is a school district. The data consists of 656 observations - all for the 2020/2021 school year. The data provides characteristics of students in each district (by income, gender, race) as well as 4 additional variables regarding student technology access.


## Preprocessing

```{r setup, include=FALSE}
setwd("/Users/rafa/Desktop/UIC/AI & ML/Rafa_Gasymova_Final_Project")
library(tidyverse)
library(tidymodels)
library("readxl")
library(DALEXtra)
library("pastecs")
library("rmarkdown")
theme_set(theme_bw())
data = read_excel("technology_data.xlsx")
```

### Renaming columns

Tech-related variables had long names and needed to be renamed.

```{r}
paged_table(data)

# renaming columns 
data = 
  data %>% 
    rename(
      `one-to-one` =
        "1. Is your district one-to-one at all grade levels? For this survey, \"one-to-one\" refers to districts providing each student with an instructional device like a laptop, tablet, and Chromebook.", 
      `# devices` = "Number of Devices for Students",
      `# can't connect` =
        "How many students are NOT able to connect to the internet from their home?", 
      `# hotspots` =
        "How many cellular wifi hotspots and cellular-connected devices does the district have available for student use for digital remote learning?",
      `Total # students` = `PreK-12`) %>% 
  mutate(`one-to-one` = tolower(`one-to-one`)) %>% 
  mutate(`one-to-one` = as_factor(case_when(
    `one-to-one` == "yes" ~ 1, #encode as a dummy variable
    `one-to-one` == "no" ~ 0
  )))

```


### Getting rid of < in data

Instead of reporting exact numbers, data inputs have a character value "<10" in cases where the value is less than 10. In order to operate with numeric variables, we need to get rid of the "<" sign. However, we acknowledge that this would add some bias into the final results.

```{r, warning=FALSE}
# getting rid of the < sign 
data[c("Homeless", "English Learner", "Hispanic", 
            "Native American/Alaskan", "Asian", "African American", 
            "Pacific Islander","White", "Two or More", "# devices", 
            "# can't connect", "# hotspots")] <- lapply(
              data[c("Homeless", "English Learner", "Hispanic", 
                          "Native American/Alaskan", "Asian", 
                          "African American", "Pacific Islander","White", 
                          "Two or More", "# devices", "# can't connect", 
                          "# hotspots")], 
              function(x) as.numeric(gsub("<", "", x)))
```

### Percentages and proportions instead of count
After running the initial analysis, we noticed a problem: we're introducing bias if we simply take count of students in the dataset. Access to technology defined as the number of accessible devices relates directly to the number of children in the school. So bigger schools are more likely to have more devices. That is why we opted towards percentages and proportions.

```{r}
data = 
  data %>% 
  mutate(`% Low Income` = `Low Income` / `Total # students` * 100,
         `% Homeless` = `Homeless` / `Total # students` * 100,
         `% English Learner` = `English Learner` / `Total # students` * 100, 
         `% Female` = `Female` / `Total # students` * 100, 
         `% Male` = `Male` / `Total # students` * 100,
         `% Hispanic` = `Hispanic` / `Total # students` * 100,
         `% Native American/Alaskan` = `Native American/Alaskan` / `Total # students` * 100,
         `% Asian` = `Asian` / `Total # students` * 100,
         `% African American` = `African American` / `Total # students` * 100, 
         `% Pacific Islander` = `Pacific Islander` / `Total # students` * 100,
         `% White` = `White` / `Total # students` * 100, 
         `% Two or More` = `Two or More` / `Total # students` * 100, 
         `# devices per student` = `# devices` / `Total # students`,
         `% can't connect` = `# can't connect` / `Total # students` * 100, 
         `# hotspots per student` = `# hotspots` / `Total # students`)
```


## Exploratory Data Analysis

### Identifying missing values

Dataset includes a total of 14 districts where data on number of devices and number of hotspots is
unavailable. The source does not explain the reason behind this.

```{r}
data_na = 
  data %>% 
  filter(if_any(everything(), is.na))

paged_table(
  data_na %>% 
  select(District, `County Name`, `Total # students`, `# can't connect`, `# hotspots`)
)
```

NA values represent 2% of all the observations, which is a small percentage.
```{r}
nrow(data_na) / nrow(data) * 100
```

### Descriptive statistics 

```{r}
data_stat =
  data %>% 
  keep(is.numeric) %>% 
  select(`Total # students`,
         `% Low Income`,
         `% Homeless`,
         `% English Learner`, 
         `% Female`, 
         `% Male`,
         `% Hispanic`,
         `% Native American/Alaskan`,
         `% Asian`,
         `% African American`, 
         `% Pacific Islander`,
         `% White`, 
         `% Two or More`, 
         `# devices per student`,
         `% can't connect`, 
         `# hotspots per student`,
         `# devices`, 
         `# can't connect`, 
         `# hotspots`
  )

data_stat = stat.desc(data_stat)

options(scipen=999)
data_stat = data_stat[-c(1, 2, 3, 10, 11, 12), ]
paged_table(data_stat)
```

Some notes about descriptive statistics:

  1. The mean and median number of devices per student is around 1
  
  2. The gap between schools is represented by the standard deviation The percentage of students who cannot connect from home varies by about 13% across school districts, while the number of devices - by around 0.5 per student.

### Density
    
```{r, warning=FALSE}
data %>%
  select(`Total # students`,
           `% Low Income`,
           `% Homeless`,
           `% English Learner`, 
           `% Female`, 
           `% Male`,
           `% Hispanic`,
           `% Native American/Alaskan`,
           `% Asian`,
           `% African American`, 
           `% Pacific Islander`,
           `% White`, 
           `% Two or More`, 
           `# devices per student`,
           `% can't connect`, 
           `# hotspots per student`) %>%                    
    gather() %>%                             
    ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +   
    geom_density()  
```

*Demographic variables:*

    1. We can see that gender variables have close to normal distribution.
    
    2. Also close to normal are the distributions of income variables - percentage of low income students.
    
    3. When it comes to race variables: density plot is left-skewed only in the case of white students (more often we can see high percentages of white students), but is right skewed in the case of all the minority groups (more often their percentage is low).
    
*Technology variables:*

    1. Number of devices per students: most often schools provide about exactly 1 device per student - again emphasizing that schools that do not perform well are not so common and can be helped specifically.
    
    2. Percentage of students who can't connect: variable is right skewed, meaning their percentage is usually low (below 25 percent)
  
## Classification model

First model attempts to predict whether a school is considered "one-to-one" (meaning able to provide each student with an instructional device like a laptop, tablet, and Chromebook) based on demographic variables.

```{r}
set.seed(1)

#split
split = initial_split(data, prop = 0.9)
train = training(split)
test = testing(split)

#recipe
classification_recipe =
  recipe(`one-to-one` ~ `% Female` + `% Homeless` + `% Low Income` + `% Hispanic` + `% Asian` + `% African American` + `% White`, data=train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_unknown(all_nominal_predictors()) %>% 
  prep()

paged_table(
  classification_recipe %>%
  summary())

paged_table(
  bake(classification_recipe, train)
)
```

### Screening multiple models
To choose the best model, we screened multiple classification models with different tuned parameters to choose the one model with the best performance. 
For this purpose we screened Random Forest, KNN and Boost Tree. The Boost Tree had the best performance.

```{r, message=FALSE}
resamples = rsample::vfold_cv(train, 3) 

# RANDOM FOREST
rf_spec = 
  rand_forest(mtry = tune(),
              min_n = tune(),
              trees = 250) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

# KNN
knn_spec =
   nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %>% 
   set_engine("kknn") %>% 
   set_mode("classification")

# BOOST TREE
xgb_spec = 
   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), 
              min_n = tune(), sample_size = tune(), trees = tune()) %>% 
   set_engine("xgboost") %>% 
   set_mode("classification")


my_set = workflow_set(
  preproc = list(reg_recp = classification_recipe),
  models = list(random_forest = rf_spec,
                KNN = knn_spec,
                boosting = xgb_spec)
)

grid_ctrl =
   control_grid(
      save_pred = FALSE,
      save_workflow = FALSE
   )

grid_results =
   my_set %>%
   workflow_map(
      seed = 1503,
      resamples = resamples,
      grid = 5,
      control = grid_ctrl,
      verbose = TRUE
   )

paged_table(grid_results)

autoplot(
   grid_results,
   rank_metric = "accuracy",  
   metric = "accuracy",       
   select_best = TRUE    
)
```


Best performing parameters for the boost tree.
```{r}
#getting the best model
best_results = 
   grid_results %>% 
   extract_workflow_set_result("reg_recp_boosting") %>% 
   select_best(metric = "accuracy")

paged_table(best_results)
```

### Accuracy and ROC for the Classification model

We were able to achieve accuracy of about 0.77 which is not bad. This means our model has some predictive power.
```{r}
library(yardstick)

best_results_fit_clas = 
   grid_results %>% 
   extract_workflow("reg_recp_boosting") %>% 
   finalize_workflow(best_results) %>% 
   fit(train)

preds = best_results_fit_clas %>% augment(train)

paged_table(preds %>%                   
  accuracy(truth = `one-to-one`, estimate=.pred_class))

preds %>%                   
  roc_curve(truth = `one-to-one`, estimate=.pred_0) %>%
  autoplot()
```


### Global Explanation/Variable Importance

Looking at variable importance allows us to see which exact features were the most important in making the predictions. The resulting visualization shows that % Asian, % White, % Low income are the most crucial features for the prediction (in that order). If we get rid of them - the model performs worse. 

```{r, message=FALSE}
data1 =
  data %>% 
  select(`one-to-one`, `% Female`, `% Homeless`, `% Low Income`, `% Hispanic`, `% Asian`, `% African American`, `% White`) %>% 
  mutate(`one-to-one` = as.numeric(`one-to-one`))

explain_reg = explain_tidymodels(
  best_results_fit_clas,
  data = data1,
  y = data1$`one-to-one`,
  verbose=TRUE
)

vip_model = model_parts(explain_reg)

plot(vip_model)
```

## Regression model
For the sake of testing out different approaches, let's construct a similar model - now a regression model - trying to predict a number of devices per student in a school using the same demographic features.

### Screening different models
Here we screen three regression models: Linear Regression, Random Forest and a Boosted Tree. Linear regression, interestingly enough showed the lowest RMSE (however, the results are very close to each other - thus, the best performing model might change with a rerun).

> It is interesting how well the models are performing overall, despite not that many variables being used and ONLY taking in socio-economic and racial characteristics of students. This within itself is a part of the answer to our research question. 

```{r, message=FALSE}
set.seed(5)

# split
split = initial_split(data, prop = 0.9)
train = training(split)
test = testing(split)

resamples = rsample::vfold_cv(train, 3)

# recipe
regression_recipe = 
  recipe(`# devices per student` ~ `% Female` + `% Homeless` + `% Low Income` + `% Hispanic` + `% Asian` + `% African American` + `% White`, data=train) %>% 
  step_unknown(all_nominal_predictors()) %>% 
  prep()

paged_table(bake(regression_recipe, train))

# LINEAR REGRESSION
linear_reg_spec = 
   linear_reg(penalty = tune(),
              mixture = tune()) %>% 
   set_engine("glmnet")

# RANDOM FOREST
rf_spec = 
   rand_forest(mtry = tune(),
               min_n = tune(),
               trees = 1000) %>% 
   set_engine("ranger") %>% 
   set_mode("regression")

# BOOSTED TREE
xgb_spec = 
   boost_tree(tree_depth = tune(),
              learn_rate = tune(),
              loss_reduction = tune(), 
              min_n = tune(),
              sample_size = tune(),
              trees = tune()) %>% 
   set_engine("xgboost") %>% 
   set_mode("regression")


my_set = workflow_set(
  preproc = list(reg_recp = regression_recipe),
  models = list(linear_reg = linear_reg_spec,
                random_forest = rf_spec,
                boosting = xgb_spec)
)

grid_ctrl =
   control_grid(
      save_pred = FALSE,
      save_workflow = FALSE
   )

grid_results =
   my_set %>%
   workflow_map(
      seed = 1503,
      resamples = resamples,
      grid = 5,
      control = grid_ctrl,
      verbose = TRUE
   )
```


```{r}
paged_table(grid_results)

autoplot(
   grid_results,
   rank_metric = "rmse",  
   metric = "rmse",       
   select_best = TRUE    
)
```

Parameters of the best model.
```{r}
#getting the best model
best_results = 
   grid_results %>% 
   extract_workflow_set_result("reg_recp_linear_reg") %>% 
   select_best(metric = "rmse")

paged_table(best_results)

#fitting the model
best_results_fit_reg = 
   grid_results %>% 
   extract_workflow("reg_recp_linear_reg") %>% 
   finalize_workflow(best_results) %>% 
   fit(train)
```

### Global Explanation/Variable Importance
The regression model yielded the same result: the resulting visualization shows that % Asian, % White, % Low income are the most crucial features for the prediction (in that order). If we get rid of them - the model performs worse. 

```{r, message=FALSE}
data2 =
  data %>% 
  select(`# devices per student`, `% Female`, `% Homeless`, `% Low Income`, `% Hispanic`, `% Asian`, `% African American`, `% White`)
explain_reg = explain_tidymodels(
  best_results_fit_reg,
  data = data2,
  y = data2$`# devices per student`,
  verbose=TRUE
)
```


```{r}
vip_model = model_parts(explain_reg, loss_function = loss_root_mean_square)

plot(vip_model)
```

## Conclusions and Notes
This project represents a rather unusual application of machine learning: we did not create a machine learning pipeline for the sake of prediction alone, rather - use it for the purposes of analysis and proof of underlying trends. By looking at what a machine can predict from the given data it seems possible **to reveal the flaws in human judgment**. Both classification and regression models mostly base their predictions on the percentage of Asian and White populations in schools. This, furthermore, demonstrates how biases can be engraved into data. If we were to ask an algorithm to assign Illinois School Districts with devices for next year based on previous history, it would most likely disproportionately favor the schools with higher percentages of Asian and White populations. Thus, hopefully, this project is able to demonstrate how crucial it is to consider the flaws of existing data and the limits of ML applications.


